@Proceedings{AIDBEI-2021,
    booktitle = {Proceedings of 2nd Workshop on Diversity in Artificial Intelligence (AIDBEI)},
    name = {Artificial Intelligence Diversity, Belonging, Equity, and Inclusion},
    shortname = {AIDBEI},
    editor = {Deepti Lamba and William H. Hsu},
    volume = {142},
    year = {2021},
    start = {2021-02-09},
    end = {2021-02-09},
    published = {2021-09-16},
    url = {http://diverseinai.com/},
    address = {Virtual},
    
}
@InProceedings{chen21,
    title = {PreDefense: Defending Underserved AI Students and Researchers from Predatory Conferences},
    author = {Chen, Thomas Y.},   
    pages = {1-6},
    abstract = {Mentorship in the AI community is crucial to maintaining and increasing diversity, especially with respect to fostering the academic growth of underserved students. While the research process itself is important, there is not sufficient emphasis on the submission, presentation, and publication process, which is a cause for concern given the meteoric rise of predatory scientific conferences, which are based on profit only and have little to no peer review. These conferences are a direct threat to integrity in science by promoting work with little to no scientific merit. However, they also threaten diversity in the AI community by marginalizing underrepresented groups away from legitimate conferences due to convenience and targeting mechanisms like e-mail invitations. Due to the importance of conference presentation in AI research, this very specific problem must be addressed through direct mentorship. In this work, we propose PreDefense, a mentorship program that seeks to guide underrepresented students through the scientific conference and workshop process, with an emphasis on choosing legitimate venues that align with the specific work that the students are focused in and preparing students of all backgrounds for future successful, integrous AI research careers.}
}


@InProceedings{monroewhite21,
    title = {Waking up to Marginalization: Public Value Failures in Artificial Intelligence and Data Science},
    author = {Monroe-White, Thema and Marshall, Brandeis and Contreras-Palacios, Hugo}, 
    pages = {7-21},
    abstract = {Data science education is increasingly becoming an integral
    part of many educational structures, both informal and formal.
    Much of the attention has been on the application of AI
    principles and techniques, especially machine learning,
    natural language processing and predictive analytics. While
    AI is only one phase in the data science ecosystem, we must
    embrace a fuller range of job roles that help manage AI
    algorithms and systems — from the AI innovators and architects
    (in CS, Math and Statistics) to the AI technicians
    and specialists (in CS, IT and IS). Also, it’s important that
    we better understand the current state of the low participation
    and representation of minoritized groups that further
    stifles the accessibility and inclusion efforts. However, how
    we learn and what we learn is highly dependent on who we
    are as learners. In this paper, we examine demographic disparities
    by race/ethnicity and gender within the information
    systems educational infrastructure from an evaluative perspective.
    More specifically, we adopt intersectional methods
    and apply the theory of public value failure to identify
    learning gaps in the fast-growing field of data science. National
    datasets of Master’s and Doctoral graduate
    students in IS, CS, Math and Statistics are used to create an
    “institutional parity score” which calculates field-specific
    representation by race/ethnicity and gender in data science
    related fields. We conclude by showcasing bias creep including
    the situational exclusion of individuals from access
    to the broader information economy, be it access to technologies
    and data or access to participate in the data workforce
    or data enabled-economic activity. Policy recommendations
    are suggested to curb and reduce this marginalization
    within information systems and related disciplines.}
}





@InProceedings{ghosh21,
    title = {Characterizing Intersectional Group Fairness with Worst-Case Comparisons},
    author = {Ghosh, Avijit and Genuit, Lea and Reagan, Mary},   
    pages = {22-34},
    abstract = {Machine Learning or Artificial Intelligence algorithms have gained
    considerable scrutiny in recent times owing to their propensity towards imitating and amplifying existing prejudices in society. This has led to a niche but growing body of work that identifies and attempts to fix these biases. A first step towards making these algorithms more fair is designing metrics that measure unfairness. Most existing work in this field deals with either a binary view of fairness (protected vs. unprotected groups) or politically defined categories (race or gender). Such categorization misses the important nuance of intersectionality - biases can often be amplified in subgroups that combine membership from different categories, especially if such a subgroup is particularly underrepresented in historical platforms of opportunity. In this paper, we discuss why fairness metrics need to be looked at under the lens of intersectionality, identify existing work in intersectional fairness, suggest a simple worst case comparison method to expand the definitions of existing group fairness metrics to incorporate intersectionality, and finally conclude with the social, legal and political framework to handle intersectional fairness in the modern context.}
}
 
@InProceedings{rivas21,
    title = {Working Set Selection to Accelerate SVR Training},
    author = {Rivas, Pablo},   
    pages = {35-38},
    abstract = {With the increasing demand for robust and resilient machine learning models, support vector machines (SVMs) are regaining attention. One of the significant problems in SVMs is finding the support vectors as soon as possible during the optimization process. This paper describes a methodology to accelerate the training by making certain assumptions on the data and find the support vectors near the convex hull of every class group. Results suggest that the methodology can provide an advantage over traditional training for larger datasets with specific statistical properties. We focus on the particular case of support vector machines for regression.}
}  

@InProceedings{freire21,
    title = {Measuring Diversity of Artificial Intelligence Conferences},
    author = {Freire, Ana and Porcaro, Lorenzo and G\'{o}mez, Emilia},   
    pages = {39-50},
    abstract = {The lack of diversity of the Artificial Intelligence (AI) field is nowadays a concern, and several initiatives such as funding schemes and mentoring programs have been designed to overcome it. However, there is no indication on how these initiatives actually impact AI diversity in the short and long term. This work studies the concept of diversity in this particular context and proposes a small set of diversity indicators (i.e. indexes) of AI scientific events. These indicators are designed to quantify the diversity of the AI field and monitor its evolution. We consider diversity in terms of gender, geographical location and business (understood as the presence of academia versus industry). We compute these indicators for the different communities of a conference: authors, keynote speakers and organizing committee. From these components we compute a summarized diversity indicator for each AI event. We evaluate the proposed indexes for a set of recent major AI conferences and we discuss their values and limitations.}
}

   


 